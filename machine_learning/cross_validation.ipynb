{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation: evaluating estimator performance\n",
    "\n",
    "### Reference\n",
    "- http://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why should we use Cross-validation?\n",
    "- merge `training set` and `validation set` instead of partitioning the available data into three sets, so we have much more data for training;\n",
    "- evaluate the performance of our methods by only using training set, so we can avoid to 'leak' the knowledge about the test set into the model;\n",
    "- select hyperparameters.\n",
    "\n",
    "When evaluating different settings (“hyperparameters”) for estimators, such as the `C` setting that must be manually set for an SVM, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic steps\n",
    "A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”:\n",
    "- A model is trained using `k - 1` of the folds as training data;\n",
    "- the resulting model is validated on the remaining part of the data (i.e. it is used as a test set to compute a performance measure such as accuracy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with raw data\n",
    "\n",
    "In scikit-learn a random split into training and test sets can be quickly computed with the train_test_split helper function. Let’s load the iris data set to fit a linear support vector machine on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((150, 4), (150,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "iris.data.shape, iris.target.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now quickly sample a training set while holding out 40% of the data for testing (evaluating) our classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96666666666666667"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, test_size=0.4, random_state=0)\n",
    "\n",
    "X_train.shape, y_train.shape\n",
    "\n",
    "X_test.shape, y_test.shape\n",
    "\n",
    "\n",
    "clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)  # 在test上计算分类的准确性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Computing cross-validated metrics\n",
    "\n",
    "The simplest way to use cross-validation is to call the cross_val_score helper function on the estimator and the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example demonstrates how to estimate the accuracy of a linear kernel support vector machine on the iris dataset by splitting the data, fitting a model and computing the score 5 consecutive times (with different splits each time):\n",
    "\n",
    "- 下面计算score的方式可以有多种选择：http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.96666667,  1.        ,  0.96666667,  0.96666667,  1.        ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "clf = svm.SVC(kernel='linear', C=1)\n",
    "# 基本用法：模型 + X + y + 交叉验证的倍数 + 模型评价的方式\n",
    "scores = cross_val_score(clf, iris.data, iris.target, cv=5, scoring='accuracy')\n",
    "scores  # 5重交叉验证，就会有5个评价结果（基于每次测试集计算得到），最终可以取均值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean score and the 95% confidence interval of the score estimate are hence given by:\n",
    "- MARK: calculate confidence interval\n",
    "\n",
    "<img src=\"https://mathbitsnotebook.com/Algebra2/Statistics/normalEmp2.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.98 (+/- 0.03)\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data transformation with held out data\n",
    "\n",
    "- 在进行预测时，对数据进行预处理的方式（例如标准化、缩放等）也应该与训练数据保持一致\n",
    "- Dataset transformations，a big topic: http://scikit-learn.org/stable/data_transforms.html#data-transforms\n",
    "\n",
    "Just as it is important to test a predictor on data held-out from training, preprocessing (such as standardization, feature selection, etc.) and similar data transformations similarly should be learnt from a training set and applied to held-out data for prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.93333333333333335"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    iris.data, iris.target, test_size=0.4, random_state=0)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_transformed = scaler.transform(X_train)\n",
    "clf = svm.SVC(C=1).fit(X_train_transformed, y_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "clf.score(X_test_transformed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 The cross_validate function and multiple metric evaluation\n",
    "\n",
    "The `cross_validate` function differs from `cross_val_score` in two ways -\n",
    "\n",
    "- It allows specifying multiple metrics for evaluation.\n",
    "- It returns a dict containing training scores, fit-times and score-times in addition to the test score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The multiple metrics can be specified either as a list, tuple or set of predefined scorer names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']\n",
      "[ 0.96666667  1.          0.96666667  0.96666667  1.        ] [ 0.96969697  1.          0.96969697  0.96969697  1.        ]\n",
      "[ 0.00057602  0.00033641  0.0002985   0.00032592  0.00029635] [ 0.00103164  0.00073767  0.00072742  0.00075459  0.00073671]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "scoring = ['precision_macro', 'recall_macro']\n",
    "clf = svm.SVC(kernel='linear', C=1, random_state=0)\n",
    "scores = cross_validate(clf, iris.data, iris.target, scoring=scoring, cv=5, return_train_score=False)\n",
    "\n",
    "print(sorted(scores.keys()))\n",
    "\n",
    "print(scores['test_recall_macro'], scores['test_precision_macro'])\n",
    "\n",
    "print(scores['fit_time'], scores['score_time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of `cross_validate` using a single metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fit_time', 'score_time', 'test_score', 'train_score']\n",
      "[ 1.          0.96491228  0.98039216] [ 0.98095238  1.          0.99047619]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_validate(clf, iris.data, iris.target, scoring='precision_macro', return_train_score=True)\n",
    "print(sorted(scores.keys()))\n",
    "\n",
    "print(scores['test_score'], scores['train_score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Obtaining predictions by cross-validation\n",
    "\n",
    "The function `cross_val_predict` has a similar interface to `cross_val_score`, but returns, for each element in the input, the prediction that was obtained for that element when it was in the test set. Only cross-validation strategies that assign all elements to a test set exactly once can be used (otherwise, an exception is raised).\n",
    "\n",
    "These prediction can then be used to evaluate the classifier:\n",
    "\n",
    "- 使用已经训练好的模型预测？那么CV在这里有什么作用呢？？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=1, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=0, shrinking=True,\n",
      "  tol=0.001, verbose=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.97333333333333338"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import metrics\n",
    "\n",
    "print(clf)\n",
    "predicted = cross_val_predict(clf, iris.data, iris.target, cv=10)\n",
    "metrics.accuracy_score(iris.target, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Cross-validation iterators for i.i.d. data\n",
    "\n",
    "Assuming that some data is Independent and Identically Distributed (i.i.d.) is making the assumption that all samples stem from the same generative process and that the generative process is assumed to have no memory of past generated samples.\n",
    "\n",
    "The following cross-validators can be used in such cases.\n",
    "\n",
    "##### NOTE\n",
    "\n",
    "While i.i.d. data is a common assumption in machine learning theory, it rarely holds in practice. If one knows that the samples have been generated using a time-dependent process, it’s safer to use a time-series aware cross-validation scheme Similarly if we know that the generative process has a group structure (samples from collected from different subjects, experiments, measurement devices) it safer to use group-wise cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 K-fold\n",
    "\n",
    "KFold divides all the samples in k groups of samples, called folds (if k = n, this is equivalent to the Leave One Out strategy), of equal sizes (if possible). The prediction function is learned using k - 1 folds, and the fold left out is used for test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of 2-fold cross-validation on a dataset with 4 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3] [0 1]\n",
      "[0 1] [2 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "X = [\"a\", \"b\", \"c\", \"d\"]\n",
    "kf = KFold(n_splits=2)\n",
    "for train, test in kf.split(X):\n",
    "    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each fold is constituted by two arrays: the first one is related to the training set, and the second one to the test set. Thus, one can create the training/test sets using numpy indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0., 0.], [1., 1.], [-1., -1.], [2., 2.]])\n",
    "y = np.array([0, 1, 0, 1])\n",
    "X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Repeated K-Fold\n",
    "\n",
    "RepeatedKFold repeats K-Fold n times. It can be used when one requires to run KFold n times, producing different splits in each repetition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of 2-fold K-Fold repeated 2 times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3] [0 1]\n",
      "[0 1] [2 3]\n",
      "[0 2] [1 3]\n",
      "[1 3] [0 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "random_state = 12883823\n",
    "rkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=random_state)\n",
    "for train, test in rkf.split(X):\n",
    "    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Leave One Out (LOO)\n",
    "\n",
    "- LOO也是一种交叉验证\n",
    "\n",
    "LeaveOneOut (or LOO) is a simple cross-validation. Each learning set is created by taking all the samples except one, the test set being the sample left out. Thus, for n samples, we have n different training sets and n different tests set. This cross-validation procedure does not waste much data as only one sample is removed from the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3] [0]\n",
      "[0 2 3] [1]\n",
      "[0 1 3] [2]\n",
      "[0 1 2] [3]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "\n",
    "X = [1, 2, 3, 4]\n",
    "loo = LeaveOneOut()\n",
    "for train, test in loo.split(X):\n",
    "    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of accuracy, LOO often results in high variance as an estimator for the test error. Intuitively, since n - 1 of the n samples are used to build each model, models constructed from folds are virtually identical to each other and to the model built from the entire training set.\n",
    "\n",
    "However, if the learning curve is steep for the training size in question, then 5- or 10- fold cross validation can overestimate the generalization error.\n",
    "\n",
    "As a general rule, most authors, and empirical evidence, suggest that 5- or 10- fold cross validation should be preferred to LOO.\n",
    "\n",
    "- Q: 如果5-或10-倍交叉验证会高估泛化误差，LOO方法不应该会更高么？？？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Leave P Out (LPO)\n",
    "\n",
    "`LeavePOut` is very similar to `LeaveOneOut` as it creates all the possible training/test sets by removing p samples from the complete set. For n samples, this produces ${n \\choose p}$ train-test pairs. Unlike LeaveOneOut and KFold, the test sets will overlap for  p > 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of Leave-2-Out on a dataset with 5 samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4] [0 1]\n",
      "[1 3 4] [0 2]\n",
      "[1 2 4] [0 3]\n",
      "[1 2 3] [0 4]\n",
      "[0 3 4] [1 2]\n",
      "[0 2 4] [1 3]\n",
      "[0 2 3] [1 4]\n",
      "[0 1 4] [2 3]\n",
      "[0 1 3] [2 4]\n",
      "[0 1 2] [3 4]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import LeavePOut\n",
    "\n",
    "X = np.ones(5)\n",
    "lpo = LeavePOut(p=2)\n",
    "for train, test in lpo.split(X):\n",
    "    print(\"%s %s\" % (train, test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Cross-validation iterators for grouped data\n",
    "\n",
    "#### Leave One Group Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
