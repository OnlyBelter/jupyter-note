{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SVM do regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wiki [link](https://en.wikipedia.org/wiki/Support_vector_machine#Regression)\n",
    "\n",
    "A version of SVM for regression was proposed in 1996 by Vladimir N. Vapnik, Harris Drucker, Christopher J. C. Burges, Linda Kaufman and Alexander J. Smola. This method is called support vector regression (SVR). The model produced by support vector classification (as described above) depends only on a subset of the training data, because the cost function for building the model does not care about training points that lie beyond the margin. Analogously, the model produced by SVR depends only on a subset of the training data, because the cost function for building the model ignores any training data close to the model prediction. Another SVM version known as least squares support vector machine (LS-SVM) has been proposed by Suykens and Vandewalle.\n",
    "\n",
    "Training the original SVR means solving\n",
    "\n",
    "minimize ${\\displaystyle {\\frac {1}{2}}\\|w\\|^{2}}$\n",
    "\n",
    "subject to ${\\displaystyle {\\begin{cases}y_{i}-\\langle w,x_{i}\\rangle -b\\leq \\varepsilon \\\\\\langle w,x_{i}\\rangle +b-y_{i}\\leq \\varepsilon \\end{cases}}} $\n",
    "where ${\\displaystyle x_{i}}$ is a training sample with target value ${\\displaystyle y_{i}}$. The inner product plus intercept ${\\displaystyle \\langle w,x_{i}\\rangle +b}$ is the prediction for that sample, and ${\\displaystyle \\varepsilon }$ is a free parameter that serves as a threshold: all predictions have to be within an ${\\displaystyle \\varepsilon }$ range of the true predictions. Slack variables are usually added into the above to allow for errors and to allow approximation in the case the above problem is infeasible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Svr_epsilons_demo.svg\">\n",
    "\n",
    "$\\varepsilon$越大，允许的误差范围就越大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- http://www.robots.ox.ac.uk/~az/lectures/ml/\n",
    "- http://www.svms.org/regression/SmSc98.pdf\n",
    "- http://svms.org/tutorials/Gunn1998.pdf\n",
    "- https://cn.mathworks.com/help/stats/understanding-support-vector-machine-regression.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example\n",
    "- http://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html#sphx-glr-auto-examples-svm-plot-svm-regression-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatically created module for IPython interactive environment\n"
     ]
    }
   ],
   "source": [
    "print(__doc__)\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# #############################################################################\n",
    "# Generate sample data\n",
    "X = np.sort(5 * np.random.rand(40, 1), axis=0)\n",
    "y = np.sin(X).ravel()\n",
    "\n",
    "# #############################################################################\n",
    "# Add noise to targets\n",
    "y[::5] += 3 * (0.5 - np.random.rand(8))\n",
    "\n",
    "# #############################################################################\n",
    "# Fit regression model\n",
    "svr_rbf = SVR(kernel='rbf', C=1e3, gamma=1, epsilon=0.2)\n",
    "svr_lin = SVR(kernel='linear', C=1e3)\n",
    "svr_poly = SVR(kernel='poly', C=1e3, degree=2)\n",
    "y_rbf = svr_rbf.fit(X, y).predict(X)\n",
    "y_lin = svr_lin.fit(X, y).predict(X)\n",
    "y_poly = svr_poly.fit(X, y).predict(X)\n",
    "\n",
    "# #############################################################################\n",
    "# Look at the results\n",
    "lw = 2\n",
    "plt.scatter(X, y, color='darkorange', label='data')\n",
    "plt.plot(X, y_rbf, color='navy', lw=lw, label='RBF model')\n",
    "plt.plot(X, y_lin, color='c', lw=lw, label='Linear model')\n",
    "plt.plot(X, y_poly, color='cornflowerblue', lw=lw, label='Polynomial model')\n",
    "plt.xlabel('data')\n",
    "plt.ylabel('target')\n",
    "plt.title('Support Vector Regression')\n",
    "plt.legend()\n",
    "plt.savefig('example_SVR_4.png', dpi=200)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.26676326,  0.24908617,  0.3547268 ,  0.60508501,  0.80558322,\n",
       "        0.73862292,  0.99971819,  0.99913522,  0.98216893,  0.96929906,\n",
       "        0.67569163,  0.8080949 ,  0.64030547,  0.63822945,  0.60885156,\n",
       "        0.90545958,  0.52432397,  0.50301796,  0.43150228,  0.28310435,\n",
       "        1.08084544,  0.25088738, -0.02288449, -0.36260628, -0.49433258,\n",
       "        0.5098226 , -0.58816107, -0.66524594, -0.67629795, -0.69038867,\n",
       "       -1.95706922, -0.92627641, -0.937215  , -0.93743767, -0.95317115,\n",
       "       -0.71257359, -0.98427415, -0.99108693, -0.99709302, -0.99462839])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用以前做线性回归的数据，尝试SCR，了解各个超参数的含义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
